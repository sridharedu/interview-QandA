## Observability Interview Questions (Senior Developer/Architect)

1.  Reflecting on your experience with high-throughput systems, perhaps at Intralinks or Bank of America, describe a scenario where you had to significantly enhance the observability of an existing critical service. What were the initial gaps, how did you identify them, and what specific logging, metrics, and tracing strategies (e.g., using ELK, Prometheus, or custom solutions) did you implement to improve visibility and troubleshooting capabilities?
2.  When designing the observability for a microservices architecture on AWS, similar to what you might have encountered in recent projects, how do you decide between using managed services like AWS CloudWatch (Logs, Metrics, X-Ray) versus setting up and managing your own stack (e.g., ELK, Prometheus, Grafana, Zipkin)? Discuss the trade-offs in terms of cost, operational overhead, feature set, and integration.
3.  Consider your work with event-driven architectures, such as the Kafka pipelines in the TOPS project. What are the unique challenges in observing these asynchronous systems compared to synchronous request-response services? How would you use metrics, logging, and distributed tracing to track message latency, identify processing bottlenecks, and ensure data integrity across Kafka topics and consumer applications?
4.  Spring Boot Actuator is a powerful tool for exposing operational information. Can you share an example where you customized or extended Actuator's capabilities? For instance, did you create custom health indicators, metrics endpoints, or integrate it with a specific part of your observability pipeline (like Prometheus) in a novel way to solve a particular monitoring challenge?
5.  Imagine you're leading the effort to establish Service Level Objectives (SLOs) for a suite of microservices. Drawing from your experience, how would you identify the appropriate Service Level Indicators (SLIs) for different types of services (e.g., user-facing APIs, backend processing jobs)? Describe the process of defining these SLOs and how you would use tools like Prometheus and Grafana to monitor adherence and manage error budgets.
6.  Distributed tracing tools like Zipkin are invaluable for debugging. Describe a complex issue you troubleshooted where the root cause was not immediately obvious from logs or metrics alone, but a distributed trace helped you pinpoint the problem. What specific details in the trace (e.g., span timings, tags, parent-child relationships across service boundaries) were key to your diagnosis?
7.  When implementing a centralized logging solution like the ELK Stack, what are the common pitfalls you've encountered or would anticipate, particularly concerning log ingestion at scale, data parsing (e.g., with Logstash Grok filters), and Elasticsearch performance tuning (indexing, sharding, retention policies)? How have you addressed these?
8.  Alerting is a critical component of observability, but alert fatigue can be a major problem. Based on your experience, what strategies have you found effective in designing an alerting system (perhaps using Prometheus Alertmanager or AWS CloudWatch Alarms) that is both sensitive to real issues and minimizes false positives? How do you decide what warrants an alert versus just being a metric on a dashboard?
9.  You're tasked with improving the observability of a critical third-party system that your application integrates with, but you have limited visibility into its internal workings. How would you approach monitoring its health and performance from your application's perspective? What "black-box" monitoring techniques and tools would you employ?
10. Observability data (logs, metrics, traces) can incur significant costs, especially in a cloud environment. Describe a situation where you had to optimize the cost of your observability stack. What levers did you pull â€“ for example, adjusting data retention, implementing sampling for traces or logs, optimizing metric cardinality, or choosing different tool tiers?
11. Consider a scenario where a new feature deployment leads to unexpected performance degradation across multiple services in your microservices platform. How would you systematically use your observability tools (logs, metrics, traces, dashboards) to isolate the impact of the new feature, identify the bottleneck(s), and guide the rollback or remediation strategy?
12. When instrumenting applications for observability, there's often a debate between auto-instrumentation (e.g., using agents or libraries like Spring Cloud Sleuth) and manual instrumentation. Based on your experience, what are the pros and cons of each approach, and how do you decide when to use one over the other, or a combination of both, for effective logging, metrics collection, and tracing?
