# Core Java: Scenario-Based Interview Questions

This section contains scenario-based interview questions that a candidate like Sridhar Nandipati, with 10+ years of experience in technologies like Vert.x, Kafka, AWS, AEM, Spring Boot, and microservices, might encounter. The answers are crafted to reflect deep understanding and practical application of Core Java concepts in relevant project contexts.

---

**Question 1:**
"Imagine you're designing a critical data processing pipeline for the TOPS project, which ingests large, variable-format financial messages via Kafka. Some messages might be malformed or contain unexpected data types, leading to parsing errors. Describe your strategy for robust exception handling within this pipeline. How would you ensure data integrity for valid parts of a message, log errors effectively for troubleshooting, and prevent the entire pipeline from halting due to isolated bad messages? Consider aspects like custom exceptions, when to catch versus propagate, and potential use of dead-letter queues (DLQs)."

**Sample Answer/Experience:**
"In the TOPS project, handling diverse financial messages from Kafka was indeed a challenge. My exception handling strategy would focus on resilience, auditability, and maintainability.

First, for parsing, I'd define a set of specific custom exceptions. For example, `MalformedMessageException`, `MissingRequiredFieldException`, or `DataTypeMismatchException`, all potentially extending a common `DataParsingException`. These exceptions would carry contextual information like the Kafka topic, partition, offset, and if possible, the problematic part of the message or field name.

The core processing logic for a message would be wrapped in a `try-catch` block.
```java
// Simplified Kafka message processing
public void processFinancialMessage(String rawMessage, ConsumerRecord<String, String> record) {
    try {
        FinancialData data = parser.parse(rawMessage);
        // ... further processing and business logic ...
        persistenceService.save(data);
    } catch (DataParsingException e) {
        logger.error("Failed to parse message. Kafka Offset: {}, Topic: {}, Partition: {}. Details: {}",
                     record.offset(), record.topic(), record.partition(), e.getMessage(), e);
        // Send to Dead Letter Queue (DLQ)
        kafkaProducer.send(new ProducerRecord<>(DLQ_TOPIC, record.key(), rawMessage));
        // Optionally, if partial data is salvageable and business rules allow:
        // handlePartialData(rawMessage, e);
    } catch (PersistenceException pe) {
        logger.error("Failed to persist processed data. Kafka Offset: {}, Data: {}. Retrying if applicable or DLQing.",
                     record.offset(), data, pe);
        // Potentially retry or send original message to DLQ
        // Or if it's a systemic issue, might need a circuit breaker or different handling
    } catch (Exception ex) { // Catch-all for unexpected issues
        logger.error("Unexpected error processing message. Kafka Offset: {}. Sending to DLQ.", record.offset(), ex);
        kafkaProducer.send(new ProducerRecord<>(DLQ_TOPIC, record.key(), rawMessage));
    }
}
```

**Key aspects of the strategy:**

1.  **Specific Custom Exceptions:** As mentioned, these allow targeted `catch` blocks and convey precise error information. They help differentiate between a truly malformed message versus, say, a database connectivity issue during persistence.
2.  **Catch Specific, then General:** I'd catch my custom `DataParsingException` first. If parsing fails, the raw message along with error details (including the exception stack trace) would be logged comprehensively. The original message would then be routed to a Dead Letter Queue (DLQ) on Kafka. This ensures no data loss and allows for offline analysis and reprocessing if needed.
3.  **Data Integrity:** If a message is partially parsable and business rules allow, I might attempt to extract and process valid portions. However, for financial data, this is often risky, so the default would be to reject the entire message on a parsing error to maintain integrity. The DLQ is crucial here.
4.  **Propagation vs. Handling:** Parsing exceptions are generally handled immediately by sending to DLQ and acknowledging the original message to Kafka to prevent re-delivery. Systemic issues, like a `PersistenceException` (e.g., database down), might involve a retry mechanism with backoff for a few attempts. If retries fail, that message might also go to a DLQ, or we might halt the consumer if it's a critical, unrecoverable backend issue, to prevent cascading failures. This depends on the SLA and system design.
5.  **Logging:** Detailed, structured logging (e.g., JSON format) is essential. Including Kafka coordinates (topic, partition, offset), a unique message ID (if available), and the full exception details aids enormously in troubleshooting.
6.  **Pipeline Continuation:** By catching exceptions at the individual message processing level and routing problematic messages to a DLQ, the main pipeline can continue processing valid messages, ensuring high availability.
7.  **Monitoring and Alerting:** The DLQ depth would be monitored. Alerts would be triggered if the DLQ size grows rapidly, indicating a systemic issue with incoming data or our parsing logic.

This approach balances robustness by not crashing the pipeline, data integrity by not processing corrupt data (or parts of it unless explicitly designed for), and maintainability through clear logging and error categorization."

---

**Question 2:**
"In your experience at Intralinks, you worked with Vert.x for high-performance applications, likely involving significant network I/O and data handling, often as JSON strings or buffers. Discuss the implications of String immutability and the String Constant Pool in such a high-throughput, asynchronous environment. When would you choose `String`, `StringBuilder`, `StringBuffer`, or even byte buffers (like Vert.x `Buffer` or Netty's `ByteBuf`) for manipulating string-like data, and what performance trade-offs would you consider?"

**Sample Answer/Experience:**
"Yes, at Intralinks, our Vert.x applications handled a lot of JSON data over the network, which involved extensive string manipulation. String immutability and the String Constant Pool have interesting implications in such contexts.

**String Immutability & Pool:**
*   **Pros:** The immutability of `String` is great for thread safety, which is less of a concern within a single Vert.x event loop thread but still beneficial when passing data between verticles or to worker threads. The String Constant Pool can save memory if we receive many duplicate string literals (e.g., common JSON keys like "transactionId", "status"). Vert.x itself, or libraries like Jackson, would likely leverage this for common keys when parsing JSON.
*   **Cons:** In a high-throughput scenario, frequent string concatenations or modifications (e.g., building large JSON responses dynamically, or manipulating parts of received strings) using `String` objects can be very inefficient. Each operation creates a new `String` object, leading to increased garbage collection pressure, which can stall the event loop â€“ something we absolutely want to avoid in Vert.x.

**Choosing the Right Tool:**

1.  **`String`:**
    *   **Use Cases:** For representing fixed, unchanging string data, like configuration values, final parts of a response, or keys in maps. When receiving data that is already a complete string and only needs to be read or parsed (not modified).
    *   **Trade-offs:** Safe and simple, but very costly for modifications.

2.  **`StringBuilder`:**
    *   **Use Cases:** This was our go-to for most dynamic string construction within a single event-loop execution or a contained synchronous block of code. For example, when constructing complex JSON payloads piece by piece before sending them as a response, or when assembling log messages.
    *   **Trade-offs:** Highly performant for single-threaded modifications as it's mutable and unsynchronized. Not suitable if the buffer needs to be shared and modified across different threads without external synchronization (which is generally anti-pattern in Vert.x event loops anyway).

3.  **`StringBuffer`:**
    *   **Use Cases:** Rarely used directly in our Vert.x event loop code because synchronization is an overhead we want to avoid. If we ever had a shared mutable string resource that absolutely needed to be modified by multiple worker threads (outside the event loop), `StringBuffer` might be an option, but we'd more likely look for concurrent data structures or message passing.
    *   **Trade-offs:** Thread-safe but slower than `StringBuilder` due to synchronization.

4.  **Byte Buffers (Vert.x `Buffer`, Netty `ByteBuf`):**
    *   **Use Cases:** This was critical for performance, especially for network I/O. Vert.x uses Netty underneath, so `Buffer` is essentially a wrapper around `ByteBuf`.
        *   **Reading from Sockets/Writing to Sockets:** All network data is fundamentally bytes. Working directly with `Buffer` avoids unnecessary conversions between byte arrays and Strings, especially if the data isn't strictly a string or if we only need to inspect parts of it.
        *   **JSON Parsing/Serialization:** Libraries like Jackson can parse JSON directly from an `InputStream` (which can be backed by a `Buffer`) or serialize to an `OutputStream`. Vert.x's own `JsonObject` and `JsonArray` often work with `Buffer`s.
        *   **Zero-Copy Operations:** `Buffer` (and `ByteBuf`) supports slicing and composite buffers, which can sometimes allow for zero-copy operations when handling network data, meaning we can send or receive parts of a buffer without copying bytes in memory. This is a huge performance win.
        *   **Encoding Control:** When converting to/from strings, `Buffer` allows explicit control over character encoding (e.g., UTF-8), which is crucial for correctness.
    *   **Trade-offs:** More complex to work with than `String` for simple text manipulation. Requires careful management of reader/writer indices and buffer capacity. However, for raw I/O and performance-critical sections, the benefits are significant. We often used `Buffer.buffer()` to create initial buffers, `appendString()`, `appendBuffer()`, and then `toString("UTF-8")` only when a full string representation was needed for external systems or logging.

**Performance Example from Intralinks:**
We had a service that processed large secure files, sometimes involving searching for specific byte patterns or replacing sections. Initially, a naive approach converted chunks to strings for processing. This was slow. We refactored it to use Vert.x `Buffer` and Netty's `ByteBufProcessor` for direct byte-level operations. This dramatically reduced memory churn and CPU usage because we avoided creating many intermediate `String` objects, especially for large files where only small portions might be modified or inspected."

---

**Question 3:**
"At Herc Rentals, you worked with Adobe Experience Manager (AEM), which often involves custom components with complex data models represented by Sling Models or other POJOs. Describe a scenario where effective use of Java Generics and Collections (beyond simple `List<String>`) was crucial for creating flexible, type-safe, and maintainable AEM components or services. Discuss how you might have used bounded wildcards (`? extends T`, `? super T`) or generic methods."

**Sample Answer/Experience:**
"In AEM development at Herc Rentals, we frequently built custom components and services that dealt with various types of content or configurations. Generics and advanced Collections usage were key to making these reusable and robust.

**Scenario: A Generic Content Aggregator Service**
Imagine we needed a service that could aggregate content items from different parts of the JCR (Java Content Repository) or from different third-party sources, where these items shared some common characteristics but also had specific variations. For example, aggregating different types of promotional content (`TeaserPromo`, `ArticlePromo`, `ProductPromo`), all of which extend a base `BasePromoPojo`.

1.  **Generic Service Interface:**
    I would define a generic interface for any service that loads these items:
    ```java
    public interface ContentLoader<T extends BasePromoPojo> {
        List<T> loadContent(ResourceResolver resourceResolver, String basePath);
        // Maybe a method to process a collection of these items
        void processItems(List<? extends T> items, ItemProcessor<? super T> processor);
    }
    ```
    Here, `T extends BasePromoPojo` is a bounded type parameter, ensuring that any `ContentLoader` implementation works with objects that are at least `BasePromoPojo`.

2.  **Using Bounded Wildcards:**
    *   In `processItems(List<? extends T> items, ...)`: The `items` list uses `? extends T`. This means if I have a `ContentLoader<BasePromoPojo>`, I can pass it a `List<TeaserPromo>` or `List<ArticlePromo>` because those are subtypes. The method can safely *read* `T` (or `BasePromoPojo`) from the list. This is 'Producer Extends' (PECS).
    *   The `ItemProcessor<? super T> processor` parameter: If `T` is `TeaserPromo`, this processor could be an `ItemProcessor<TeaserPromo>` or an `ItemProcessor<BasePromoPojo>` or even `ItemProcessor<Object>`. The processor *consumes* `T` instances. This is 'Consumer Super' (PECS). This allows for more generic processors to be used.

    ```java
    // Example ItemProcessor interface
    public interface ItemProcessor<T> {
        void process(T item);
    }
    ```

3.  **Generic Methods for Utility Functions:**
    We might have utility methods within our AEM services, for example, to filter a list of components based on some criteria:
    ```java
    public <C extends Component> List<C> filterComponents(List<C> components, Predicate<C> filterCriteria) {
        return components.stream().filter(filterCriteria).collect(Collectors.toList());
    }
    // Usage:
    // List<TeaserComponent> teasers = ...;
    // List<TeaserComponent> activeTeasers = filterComponents(teasers, tc -> tc.isActive());
    ```
    This generic method `filterComponents` can work with any list of objects that implement `Component` (a common AEM interface or our custom one) and a `Predicate` for that type.

4.  **Type-Safe Data Structures in Sling Models:**
    When defining Sling Models for our AEM components, if a component could hold a collection of different but related items (e.g., a carousel with different slide types), we'd use generics.
    ```java
    @Model(adaptables = Resource.class)
    public class CarouselModel {
        @Inject // or @ChildResource
        @Named("slides")
        private List<SlidePojo> slides; // SlidePojo could be an interface, and actual items are TeaserSlide, ImageSlide etc.

        public List<SlidePojo> getSlides() { return new ArrayList<>(slides); } // Defensive copy
    }
    ```
    If `SlidePojo` was an interface, and we had different implementations like `TextSlidePojo` and `ImageSlidePojo` (both implementing `SlidePojo`), this structure would hold them. If we needed to perform operations on these where the specific type mattered, we might use `instanceof` checks followed by casting, or more elegantly, a visitor pattern.

**Benefits:**
*   **Type Safety:** Reduced `ClassCastException`s at runtime because the compiler enforces type constraints.
*   **Reusability:** Generic services like `ContentLoader` or utility methods like `filterComponents` could be reused across different components and data types without code duplication.
*   **Maintainability:** Code becomes easier to understand and refactor because the relationships between different data types are clearly defined. For instance, if we introduce a new `SpecialPromoPojo`, as long as it extends `BasePromoPojo`, it can be used with our existing `ContentLoader<BasePromoPojo>` and related processing logic with minimal changes.

By using generics effectively, we could build more abstract and flexible AEM solutions, avoiding tight coupling to concrete implementations and making the overall codebase more robust."

---

**Question 4:**
"Your work on AWS microservices, for instance at Intralinks or TOPS, likely involved making decisions about JVM memory configuration (heap size, metaspace) and Garbage Collection tuning. Describe a situation where a microservice was experiencing performance degradation or instability related to memory or GC. How did you approach diagnosing the issue, what JVM parameters did you consider adjusting, and what was the outcome? What tools were invaluable in this process?"

**Sample Answer/Experience:**
"At Intralinks, we had a Spring Boot microservice responsible for encrypting and decrypting large files as part of our secure collaboration platform. It was deployed on AWS ECS. Users reported intermittent slowness and occasional `OutOfMemoryError` (OOMEs) during peak loads when multiple large file operations were concurrent.

**Diagnosis Approach:**

1.  **Monitoring:** We first looked at AWS CloudWatch metrics for ECS, focusing on CPU utilization, memory utilization (both service and container level), and task restarts. We saw memory usage climbing steadily under load, then dropping after a restart (classic OOME symptom), and high CPU when it was struggling.
2.  **Logging:** Application logs showed OOMEs, specifically `java.lang.OutOfMemoryError: Java heap space`. We also temporarily enabled verbose GC logging using flags like `-Xlog:gc*:file=/var/log/app/gc.log:time,uptime,level,tags:filecount=10,filesize=50m`.
3.  **Heap Dumps:** We configured JVM options to generate a heap dump on OOME: `-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/mnt/app/heapdump.hprof`. After an OOME, we pulled this dump from the container.
4.  **Profiling (Local/Staging):** While harder in production immediately, we tried to reproduce similar load patterns in a staging environment with a profiler attached (like YourKit or JProfiler, though VisualVM with sampling can also be useful) to observe object allocation patterns.

**Analysis and JVM Parameter Adjustments:**

*   **Heap Dump Analysis:** Using Eclipse MAT (Memory Analyzer Tool) on the heap dump, we found that our `EncryptionStreamHandler` objects, along with large byte arrays they held for buffering file content during streaming encryption/decryption, were consuming the majority of the heap and not being garbage collected promptly. It wasn't a simple leak of one object type, but rather the sheer volume and size of objects related to concurrent file processing.
*   **Initial JVM Settings:** The service had default JVM settings with a relatively small max heap size (e.g., `-Xmx512m`).
*   **GC Log Analysis:** The GC logs (when we got them before an OOME) showed frequent minor GCs and increasing time spent in Full GCs (using G1GC which was default). The old generation was filling up rapidly.

**Changes and Considerations:**

1.  **Increase Max Heap Size (`-Xmx`):** The most immediate step was to provide more memory. We analyzed typical file sizes and concurrency, then increased `-Xmx` to a more reasonable value (e.g., `-Xmx2g`) based on the instance type in ECS. We also set `-Xms` to the same value as `-Xmx` to pre-allocate the heap and avoid pauses for heap expansion.
2.  **GC Strategy (G1GC Tuning):** We were already using G1GC.
    *   We ensured `-XX:+UseG1GC` was explicit.
    *   We looked at region size. For large objects, sometimes tuning `-XX:G1HeapRegionSize` can help, but we left it to default initially.
    *   The primary issue was the rate of allocation and promotion. We considered increasing the young generation size (`-XX:G1NewSizePercent`, `-XX:G1MaxNewSizePercent`) to allow more objects to die young, but large byte arrays from file streams would likely be promoted quickly anyway.
3.  **Application Code Review:** More importantly than just JVM tuning, we reviewed the `EncryptionStreamHandler` code.
    *   **Buffer Sizing:** We found that the internal byte buffers used for I/O were excessively large for some use cases. We made buffer sizes configurable or more adaptive.
    *   **Object Pooling:** For certain helper objects involved in the cryptographic operations, we implemented a small object pool to reduce churn, though this needs to be done carefully to avoid becoming a memory leak source itself.
    *   **Stream Handling:** Ensured all streams were meticulously closed in `finally` blocks or using `try-with-resources` to prevent resource leaks that could indirectly hold onto large buffers. We were already doing this, but it's always a key check.
4.  **Concurrency Limits:** We considered implementing a semaphore or a bounded queue at the application level to limit the number of truly *concurrent* large file operations, rather than letting the JVM struggle with an unbounded number. This provides more graceful degradation.

**Outcome:**
Increasing the heap size provided immediate relief from OOMEs. Analyzing GC logs helped confirm that G1GC was generally behaving as expected with the new heap size, but the sheer allocation rate was the main challenge. The code-level optimizations to buffer handling and reducing object churn for stream processing were crucial for long-term stability and better performance. The service became much more stable, and processing times for large files improved because less time was spent in GC, especially Full GCs.

**Tools:** AWS CloudWatch, Eclipse MAT, GCViewer (to visualize GC logs), and application logging were invaluable. JProfiler/YourKit were used in staging.

This iterative process of monitoring, diagnosing, tuning, and code review is typical for resolving such issues in microservices."

---

**Question 5:**
"Consider a scenario in the Herc Rentals AEM platform. You need to develop a custom AEM service that inspects and potentially modifies properties of various Sling Models (POJOs used to represent component data) based on a set of configurable rules. These rules might specify which properties to target by name or by a custom annotation, and how to transform their values. How might you leverage Java Reflection and/or custom Annotations to build such a dynamic and configurable service? Discuss the design, benefits, and potential pitfalls (e.g., performance, type safety)."

**Sample Answer/Experience:**
"At Herc Rentals, building dynamic AEM services that could adapt to different Sling Model structures without hardcoding property names was a common requirement. Using Java Reflection and custom annotations would be an excellent approach for this kind of rule-based property inspection and modification service.

**Design Approach:**

1.  **Custom Annotation for Targetable Properties:**
    I'd first define a custom annotation, say `@ModifiableProperty`, that can be used to mark fields within Sling Models that this service is allowed to inspect and modify.
    ```java
    @Retention(RetentionPolicy.RUNTIME) // Must be available at runtime for reflection
    @Target(ElementType.FIELD)        // Apply only to fields
    public @interface ModifiableProperty {
        String ruleSetName() default "default"; // Allows associating different rule sets
        boolean allowModification() default true;
    }
    ```

2.  **Rule Configuration:**
    The rules themselves could be defined perhaps in OSGi configuration for the service, or even as content in the JCR. A rule might look like:
    *   `modelClass: com.hercrentals.models.EquipmentDetailsModel`
    *   `propertyName: "priceInfo"` (or it could target by `@ModifiableProperty` annotation with a specific `ruleSetName`)
    *   `condition: "currentValue == null || currentValue.isEmpty()"` (simplified condition)
    *   `action: "setValue('N/A')"` or `transform: "com.hercrentals.rules.PriceTransformer"`

3.  **The Service Implementation:**
    The service would take a Sling Model object (which is just a POJO) as input.
    ```java
    public class DynamicPropertyService {

        public void inspectAndModify(Object slingModel, List<Rule> applicableRules) {
            if (slingModel == null) return;
            Class<?> modelClass = slingModel.getClass();

            for (Field field : modelClass.getDeclaredFields()) {
                // Option 1: Check for our custom annotation
                if (field.isAnnotationPresent(ModifiableProperty.class)) {
                    ModifiableProperty annotation = field.getAnnotation(ModifiableProperty.class);
                    if (annotation.allowModification()) {
                        // Process based on rules matching annotation.ruleSetName() or field name
                        processField(slingModel, field, applicableRules);
                    }
                }
                // Option 2: Directly match field names from rules (less flexible)
                // else { processFieldByName(slingModel, field, applicableRules); }
            }
        }

        private void processField(Object model, Field field, List<Rule> rules) {
            try {
                field.setAccessible(true); // Necessary if fields are private
                Object currentValue = field.get(model);

                for (Rule rule : rules) {
                    if (matchesRule(field.getName(), annotationIfExists(field), rule)) {
                        if (evaluateCondition(currentValue, rule.getCondition())) {
                            Object newValue = determineNewValue(currentValue, rule.getAction(), rule.getTransformerClass());
                            field.set(model, newValue);
                            logger.info("Property '{}' modified for model '{}'", field.getName(), model.getClass().getSimpleName());
                        }
                    }
                }
            } catch (IllegalAccessException e) {
                logger.error("Error accessing field {} in model {}", field.getName(), model.getClass().getName(), e);
            } catch (Exception e) { // Catch other exceptions like for transformer instantiation
                logger.error("Error processing field {} with rule {}: {}", field.getName(), rule.getName(), e.getMessage(), e);
            }
        }
        // ... helper methods for matchesRule, evaluateCondition, determineNewValue ...
    }
    ```

**Benefits:**

*   **Dynamic and Configurable:** The core service logic doesn't need to change if new Sling Models are introduced or if rules for existing models change. Rules and annotations drive the behavior.
*   **Decoupling:** The service is decoupled from the concrete Sling Model classes. It operates on them via reflection.
*   **Reduced Boilerplate:** Avoids writing repetitive if/else or switch statements for each model type and property.
*   **Centralized Logic:** Property modification logic based on shared rules is centralized in one service.

**Pitfalls and Mitigation:**

*   **Performance:** Reflection is slower than direct field access. For AEM components, this usually happens on content rendering or authoring actions. If this service were called extremely frequently in a tight loop, performance could be an issue.
    *   **Mitigation:** Cache reflection results (e.g., `Field` objects, method handles) if a model class is processed many times. For AEM, this is often acceptable as it's not typically in a high-frequency trading style loop. Profile if performance becomes a concern.
*   **Type Safety:** Reflection bypasses compile-time type checking to some extent. Setting a field with an incompatible type will result in a runtime error (`IllegalArgumentException`).
    *   **Mitigation:** The `determineNewValue` logic needs to be robust. If using transformer classes, ensure they handle type conversions carefully or are specific to field types. Rules should be designed with type awareness. Extensive testing is crucial.
*   **Security / Encapsulation:** `field.setAccessible(true)` breaks encapsulation by allowing access to private fields.
    *   **Mitigation:** This is often a necessary trade-off for this kind of generic framework-level service. Document its behavior clearly. The custom annotation `@ModifiableProperty` acts as an explicit opt-in by the model designer, mitigating some concerns.
*   **Refactoring Brittleness:** Renaming fields in Sling Models could break rules that target properties by name if not updated simultaneously.
    *   **Mitigation:** Prefer targeting fields via the `@ModifiableProperty` annotation where possible. If using names, ensure good test coverage to catch these issues.

This reflective approach, while powerful, needs careful design and thorough testing, but it can provide significant flexibility for dynamic content manipulation in platforms like AEM."

---

**Question 6:**
"In designing a microservices architecture, like the one you contributed to at TOPS or Intralinks using Spring Boot on AWS, a common challenge is managing shared utility code or cross-cutting concerns (e.g., custom logging, metrics, security checks) without tightly coupling services. How would you apply Object-Oriented Design principles (like SOLID, design patterns) and Core Java features (e.g., interfaces, abstract classes, generics) to create reusable libraries or modules for these concerns? Discuss how these libraries would be versioned and consumed by different microservices."

**Sample Answer/Experience:**
"This is a very relevant challenge in microservice architectures. At both TOPS and Intralinks, we aimed for autonomous services but also needed consistency in cross-cutting concerns.

**OOD Principles & Core Java Features for Reusable Libraries:**

1.  **Single Responsibility Principle (SRP) & Interface Segregation Principle (ISP):**
    *   I would design small, focused libraries (JARs) for each specific concern. For example, a `custom-logging-lib`, a `metrics-emitter-lib`, a `security-filter-lib`.
    *   Each library would expose its functionality through narrow, well-defined interfaces. For instance, `custom-logging-lib` might offer:
        ```java
        public interface AuditLogger {
            void logEvent(AuditEvent event); // AuditEvent is a specific, well-defined DTO
        }
        public interface PerformanceTracer {
            TraceContext startTrace(String operationName);
            void endTrace(TraceContext context);
        }
        ```
    This avoids bloating libraries and allows services to depend only on the interfaces they need.

2.  **Open/Closed Principle (OCP):**
    *   Libraries should be open for extension but closed for modification. We can achieve this using:
        *   **Strategy Pattern:** For concerns like custom metrics emission, the library could provide an interface (`MetricsPublisher`) and a few default implementations (e.g., `CloudWatchMetricsPublisher`, `ConsoleMetricsPublisher`). Services could provide their own implementations if needed.
        *   **Template Method Pattern:** An abstract base class could define the skeleton of an algorithm (e.g., for a common request handling flow with pre/post processing hooks), and specific services or other libraries could subclass it to provide concrete implementations for the hooks.
            ```java
            // In 'request-handler-core-lib'
            public abstract class BaseSecureRequestHandler<T, R> {
                protected abstract boolean authorizeRequest(T request);
                protected abstract R processMainLogic(T request);
                protected void postProcess(R response) { /* default no-op */ }

                public final R handleRequest(T request) {
                    if (!authorizeRequest(request)) {
                        throw new AuthorizationException("Unauthorized");
                    }
                    R response = processMainLogic(request);
                    postProcess(response);
                    return response;
                }
            }
            ```

3.  **Dependency Inversion Principle (DIP):**
    *   Libraries should depend on abstractions (interfaces), not concretions. Services consuming the library would also depend on these interfaces. Spring Boot's dependency injection would then wire in the concrete implementations provided by the library or the service itself.
    *   For example, a service needing audit logging would `@Autowired AuditLogger auditLogger;` and Spring would inject the library's implementation.

4.  **Generics for Flexibility:**
    *   For utilities like a generic API client or a data transformation utility within a library, generics would be used to provide type safety and reusability.
        ```java
        // In 'common-api-client-lib'
        public interface RestApiClient {
            <T> Optional<T> getForObject(String serviceUrl, Class<T> responseType, Map<String, String> headers);
            <R, T> Optional<T> postForObject(String serviceUrl, R requestBody, Class<T> responseType, Map<String, String> headers);
        }
        ```

5.  **Configuration:**
    *   Libraries would be configurable using Spring Boot's `@ConfigurationProperties` mechanism. For example, `custom-logging-lib` might allow configuration of log levels for audit events or output formats via application properties in the consuming microservice.

**Versioning and Consumption:**

*   **Semantic Versioning:** Each library would follow semantic versioning (MAJOR.MINOR.PATCH).
    *   PATCH: Bug fixes, backward compatible.
    *   MINOR: New features, backward compatible.
    *   MAJOR: Breaking changes.
*   **Maven/Gradle Repository:** Libraries would be published to a central artifact repository (like Nexus or Artifactory).
*   **Dependency Management:** Each microservice would declare dependencies on these libraries in its `pom.xml` or `build.gradle` with a specific version.
*   **Bill of Materials (BOM):** For larger sets of related libraries, we might create a BOM POM that defines a compatible set of versions for common libraries. Microservices then import this BOM.
*   **Careful Upgrades:** Upgrading library versions, especially MAJOR versions, would need careful planning and testing per microservice to manage breaking changes. Minor/Patch versions could often be updated more easily, but still require integration testing.
*   **Communication:** Clear release notes and communication about changes in shared libraries are crucial for all development teams.

**Example: `security-filter-lib`**
This library could provide Spring Security `Filter` implementations (e.g., a JWT authentication filter).
*   It would define an interface like `UserInfoProvider` that the consuming service *must* implement to provide user details from its own user store. The JWT filter would use this interface.
*   The filter itself would be packaged in the library.
*   The consuming service would include the library, provide the `UserInfoProvider` bean, and configure the filter in its Spring Security chain. Default configurations could be provided via `@AutoConfiguration` in the library.

By following these principles, we could create a suite of shared libraries that promote code reuse and consistency while still allowing microservices to evolve independently to a large extent."

---
